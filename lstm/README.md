# Генерация текста

- Выберите любой текст или набор данных с текстовыми объектами. Разбейте его на последовательности по предложениям, либо используйте естественное разбиение, если оно изначально было в наборе данных.
- Токенезируйте последовательности. Желательно использовать Byte-Pair Encoding. Запомните токены конца предложения, либо добавьте новый служебный токен, если последовательности нарезались не по предложениям.
- Преобразуйте их в тренировочный набор данных для задачи предсказания следующего символа по текущему префиксу последовательности.
- Выберите любой достаточно продвинутый фреймворк для глубокого обучения. Постройте архитектуру на основе LSTM слоёв для задачи предсказания следующего символа. Токены должны преобразовываться обучаемыми эмбеддингами. Например, используя nn.Embedding в PyTorch.
- Обучите построенную архитектуру. 
- Реализуйте генерацию текста на основе жадного и случайного выбора очередного токена. Случайность должна быть не равновероятной, а основываться на предсказываемых вероятностях.
- При помощи двух стратегий сгенерируйте несколько предложений используя реальные начала предложений из текста.
- Повторите пункты 5–7 для Марковской цепи.
